{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgT94x0WubIv"
   },
   "source": [
    "# A Simple Artificial Neural Network in 11 Lines\n",
    "\n",
    "The code and explanations are adapted from a blog by Andrew Trask https://iamtrask.github.io/2015/07/12/basic-python-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w-BE0D_Ful_p"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qy0rGsNpJ0pS"
   },
   "source": [
    "The following code implements an ANN and trains it to solve the XOR problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6oi36RP-uUb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.65698538 -0.64522731 -0.8600113   0.29002209]\n",
      " [-0.11121401 -0.76011788 -0.25390778  0.42609755]\n",
      " [ 0.4212753  -0.39859889  0.67257366  0.90874581]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "t = np.array([[0,1,1,0]]).T\n",
    "W1 = 2*np.random.random((3,4)) - 1\n",
    "W2 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "print(W1)\n",
    "for j in range(60000):\n",
    "    h = 1/(1+np.exp(-(np.dot(X,W1))))\n",
    "    y = 1/(1+np.exp(-(np.dot(h,W2))))\n",
    "    y_delta = (t - y)*(y*(1-y))\n",
    "    h_delta = np.dot(y_delta, W2.T) * (h * (1-h))\n",
    "    W2 += np.dot(h.T, y_delta)\n",
    "    W1 += np.dot(X.T, h_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "t = np.array([[0,1,1,0]]).T\n",
    "W1 = 2*np.random.random((3,4)) - 1\n",
    "W2 = 2*np.random.random((4,1)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "***********\n",
      "[[0.00271669]\n",
      " [0.99641162]\n",
      " [0.99583942]\n",
      " [0.00519848]]\n",
      "***********\n",
      "[[-0.98897579  0.25363973 -0.3106236  -0.6600815 ]\n",
      " [-0.56374814  0.04757171 -0.95067411 -0.27628703]\n",
      " [-0.69173401  0.27664081 -0.7694167  -0.69348932]]\n",
      "***********\n",
      "[[ 0.54990431]\n",
      " [ 0.73516547]\n",
      " [-0.26938963]\n",
      " [-0.02559256]]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(\"***********\")\n",
    "print(y)\n",
    "print(\"***********\")\n",
    "\n",
    "print(W1)\n",
    "print(\"***********\")\n",
    "\n",
    "print(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.14238645, -0.98753243, -0.27991274, -0.99500197],\n",
       "       [-0.93598625, -0.64517199, -0.34431362, -0.6127013 ],\n",
       "       [-0.42466773, -0.80132635, -0.57919529, -0.82266348]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random((3,4)) -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRMvMwGnJ_KL"
   },
   "source": [
    "Here is some code that passes an input through the network, to produce a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ivyt4Ze7uiV6"
   },
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "  '''Produce a prediction for x, using the weights defined above.'''\n",
    "  ### INPUT -> HIDDEN LAYER\n",
    "  # Multiply x by the first set of weights\n",
    "  z1 = np.dot(x,W1)\n",
    "  # Activation function\n",
    "  h = 1/(1+np.exp(-(z1)))\n",
    "\n",
    "  ### HIDDEN LAYER -> OUTPUT\n",
    "  # Multiply the hidden layer outputs by the next set of weights\n",
    "  z2 = np.dot(h,W2)\n",
    "  # Activation function\n",
    "  y = 1/(1+np.exp(-(z2)))\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Q8KaecjkvTAj",
    "outputId": "6dc3d0f8-a3f3-4428-eb97-106dd7af5d6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00465699]\n",
      "[0.99754167]\n",
      "[0.99521407]\n",
      "[0.00361097]\n"
     ]
    }
   ],
   "source": [
    "# The final input is always 1 because it corresponds with the bias parameter\n",
    "print(predict([0,0,1]))\n",
    "print(predict([0,1,1]))\n",
    "print(predict([1,0,1]))\n",
    "print(predict([1,1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ipRciAk0jGt"
   },
   "source": [
    "# Breakdown of the 11 lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[ 0.34477793 -0.8700625  -0.90486842  0.31404457]\n",
      " [ 0.93204318 -0.82879262 -0.63249823 -0.77777939]\n",
      " [ 0.20453021 -0.33803014  0.00851125 -0.14883615]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "W1 = 2*np.random.random((3,4)) - 1\n",
    "\n",
    "print(X)\n",
    "print(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.20453021, -0.33803014,  0.00851125, -0.14883615],\n",
       "       [ 1.13657339, -1.16682277, -0.62398698, -0.92661554],\n",
       "       [ 0.54930814, -1.20809264, -0.89635716,  0.16520842],\n",
       "       [ 1.48135132, -2.03688527, -1.5288554 , -0.61257097]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dot(W1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7lL-bZv1Kkg"
   },
   "source": [
    "## Line 1-2:\n",
    "\n",
    "```Python\n",
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "t = np.array([[0,1,1,0]]).T\n",
    "```\n",
    "\n",
    "The training set `X` and training labels `y`. The last input is always $1$, as it represents the bias parameter. So the inputs can actually be thought of as 2-dimensional.\n",
    "\n",
    "For example, the expected output when the input is `[0,1]` is `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "euXqNB-H10OE"
   },
   "source": [
    "## Line 3-4:\n",
    "\n",
    "```Python\n",
    "W1 = 2*np.random.random((3,4)) - 1\n",
    "W2 = 2*np.random.random((4,1)) - 1\n",
    "```\n",
    "\n",
    "These are all the parameters of the ANN. The ANN has 2 input nodes (+1 bias), a hidden layer containing 4 hidden nodes, and 1 output node.\n",
    "\n",
    "`W1` contains the weights that connect the 2+1 inputs to the 4 hidden nodes.\n",
    "\n",
    "`W2` contains the weights that connect the 4 hidden nodes to the 1 output node.\n",
    "\n",
    "The parameters are randomly initialized with mean $-1$ and variance $2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lbb92YK3Axs"
   },
   "source": [
    "## Line 5-7:\n",
    "\n",
    "```Python\n",
    "for j in range(60000):\n",
    "  h = 1/(1+np.exp(-(np.dot(X,W1))))\n",
    "  y = 1/(1+np.exp(-(np.dot(h,W2))))\n",
    "```\n",
    "\n",
    "The weights of the ANN are going to be adjusted over the course of 60,000 passes of the training set.\n",
    "\n",
    "`h` is the **output of the hidden layer.**\n",
    "\n",
    "`y` is the **output in the final layer.**\n",
    "\n",
    "`np.dot(X.W1)` is the matrix product of `X` and `W1`. For example:\n",
    ">$\\begin{pmatrix}\n",
    "    0 & 0 & 1 \\\\\n",
    "    0 & 1 & 1 \\\\\n",
    "    1 & 0 & 1 \\\\\n",
    "    1 & 1 & 1\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    w_{11} & w_{12} & w_{13} & w_{14}\\\\\n",
    "    w_{21} & w_{22} & w_{23} & w_{24} \\\\\n",
    "    w_{31} & w_{32} & w_{33} & w_{34}\n",
    "  \\end{pmatrix} = \n",
    "  \\begin{pmatrix}\n",
    "    z_{11} & z_{12} & z_{13} & z_{14} \\\\\n",
    "    z_{21} & z_{22} & z_{23} & z_{24} \\\\\n",
    "    z_{31} & z_{32} & z_{33} & z_{34} \\\\\n",
    "    z_{41} & z_{42} & z_{43} & z_{44}\n",
    "  \\end{pmatrix}$\n",
    "\n",
    "$w_{ij}$ is the weight connecting input $i$ to hidden node $j$.\n",
    "\n",
    "$z_{ij}$ represents the weighted input to hidden node $j$ when the input is example number $i$ (corresponding to a row of `X`).\n",
    "\n",
    "Each weighted input then has the activation function applied to it, which in this case is $\\sigma(z) = \\frac{1}{1 + e^{-z}}$. This gives the values in `h`.\n",
    "\n",
    "This process is repeated to get the output layer, `y`, but this time a $4 \\times 4$ matrix `h` is multiplied by a $4 \\times 1$ matrix `W2`. This gives a $4 \\times 1$ matrix, where each entry corresponds to the output, for a given input example. Ideally we want `y` to look like `t`: $\\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6uAi3pO8VP1"
   },
   "source": [
    "## Line 8\n",
    "\n",
    "```Python\n",
    "y_delta = (t - y)*(y*(1-y))\n",
    "```\n",
    "\n",
    "Lines 8 and 9 are the backpropagation step.\n",
    "\n",
    "The loss function is $L(y,t) = \\frac{1}{2}(y-t)^2$ and the activation function is $y = \\sigma(z) = \\frac{1}{1+e^{-z}}$.\n",
    "\n",
    "`y_delta` represents how the loss changes with respect to **the weighted inputs that go into the output layer**.\n",
    "\n",
    "We know by the chain rule that $\\frac{dL}{dz} = \\frac{dL}{dy} \\cdot \\frac{dy}{dz}$.\n",
    "\n",
    "Taking derivatives we find that:\n",
    "\n",
    "$\\frac{dL}{dy} = y-t$,\n",
    "\n",
    "$\\frac{dy}{dz} = \\frac{d\\sigma}{dz} = \\sigma(z)\\cdot(1-\\sigma(z)) = y(1-y)$.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$\\frac{dL}{dz} = (y-t)\\cdot y \\cdot (1-y)$.\n",
    "\n",
    "But since we want to minimize the loss, we want to take a step in the direction of $-\\frac{dL}{dz}$, which explains `y_delta`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OE2ABJ-CDSSf"
   },
   "source": [
    "## Line 9\n",
    "\n",
    "```Python\n",
    "h_delta = np.dot(y_delta, W2.T) * (h * (1-h))\n",
    "```\n",
    "\n",
    "`h_delta` represents how the loss changes with respect to **the weighted inputs that go into the hidden layer.** However, the output of the hidden layer goes through the second set of weights, `W2`, so ultimately `h_delta` must depend on `W2`. \n",
    "\n",
    "We already know how $L$ depends on the weighted inputs to the output layer, `y_delta`. We can pass those values backwards across connections between the output and hidden layer. We do this simply by multiplying `y_delta` by the weights in `W2`, and using transpose where appropriate. The resulting values describe how the loss changes with respect to the output of the hidden layer.\n",
    "\n",
    "Then, according to the chain rule, we also multiply by a factor which describes how the output of the hidden node changes with respect to the weighted input to the hidden node. We already did this: it is the derivative of the activation function $\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))$, where $h = \\sigma(z)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04jNbY2gGJ3U"
   },
   "source": [
    "## Line 10-11\n",
    "\n",
    "```Python\n",
    "W2 += np.dot(h.T, y_delta)\n",
    "W1 += np.dot(X.T, h_delta)\n",
    "```\n",
    "\n",
    "This is the gradient descent step.\n",
    "\n",
    "The second set of weights `W2` should be updated by taking a step in direction `y_delta`. The size of the step is scaled by the hidden layer output coming along that connection. The matrix product does this for every training example simultaneously, so that the weight update improves the ANN's ability to classify _every_ example.\n",
    "\n",
    "A similar calculation is done to update the weights of `W1`, this time using `h_delta` to determine the direction of the step, and the inputs from the first layer `X` to dictate the magnitude of the step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7psWsAnsL9VB"
   },
   "source": [
    "# Exercises (for your benefit, won't be marked)\n",
    "\n",
    "1. How would you modify the code to add more nodes to the hidden layer?\n",
    "\n",
    "2. How would you modify the code to add another hidden layer?\n",
    "\n",
    "3. Suppose I wanted to use a different activation function. Which lines of the code would have to change?\n",
    "\n",
    "4. Try removing a training example. If you train the network and ask it to predict a value for the missing example, does it do it correctly?\n",
    "\n",
    "5. You can measure the overall loss of the network by taking the average loss over all the training examples. Try recording this overall loss once per training loop, and plotting it with respect to the number of weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gJWMfy-1MNk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.87276665],\n",
       "       [4.91662386],\n",
       "       [5.47691733]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(W1, axis=1, keepdims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "ann_eleven_lines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
